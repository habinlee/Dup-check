GCP Datastore
- GCP Datastore environment setting process
- Notes on using GCP Datastore (Python 3)
- Testing process
Google Cloud Platform - Datastore

GCP Datastore environment setting process
- First, before installing Datastore, core files including the Google SDK need to be installed
  - The company GCP console already has its basic settings done and ready
  -  The following processes are written based on MacOS
  - https://cloud.google.com/sdk/docs/quickstarts
  - Google SDK download process
      - First, download python or check the downloaded version of python that you have
      
      - Next, using the following link, download the SDK file : https://d1.googl.com/d1/cloudsdk/channels/rapid/downloads/google-Cloud-sdk-294.0.0-darwin-x86_64.tar.gz
      Use the next command

      - After this step, use the next command to initialize

      - (Assuming that all core configuration files and the Google SDK are downloaded successfully) Use the next command on the terminal to download GCP Datastore
      














Notes on using GCP Datastore (Python 3)
- Datastore is a noSQL storage
- Required when using the Datastore API :

- Setting a ‘client’ is required to access the data stored in Datastore dynamically

- Each item/element in Datastore is sorted by a ‘kind’

  - At this point, it is important to NOT use special cases in the kind name of your Datastore storage - the reason is explained later on
  With the method above, it is possible to store a specific data format as a variable
    - It is essential when inserting data
    - When inserting data, simply use the client to put data into Datastore

- The most frequently used method of reading data from Datastore is filtering and query fetching
  - First, read all the data that is recorded in Datastore in the form of a query, and then apply specific conditions to filter the Datastore query

    - One thing to keep in mind is that it is hard to bring back a filtered query to its original state using the same query variable. To use another query with the entire data, you need to call another query onto another variable
  - Next, by saving the query that has been filtered with the appropriate conditions into a list, looping through the data can be done more efficiently

    - When saved in this form, it is easier to use in your code (loops etc)
    - When calling the query of the data., the maximum number of items to be called can be set. For example, to bring maximum of 5 items, you can do :

  - It is also possible to configure indexes in Datastore. Construct an index.yaml file and deploy it with the application.
    - Structure of the index.yaml file

    - How to deploy :

    - Example of the index applied to Datastore :

- References :
  - python-docs-samples/quickstart.py at master · GoogleCloudPlatform/python-docs-samples · GitHub
















Testing Process
- There were two big categories of testing that I proceeded with for testing the service
  - Latency difference depending on applying index configuration
    - The test was done in the local environment
    - The latency of inserting 100,000 items of data was measured
      - As the amount of insert data increased the testing process itself started to become slower so to insert  loads of data simultaneously at a fast speed, thread testing method was used
      - For this experiment, 20 threads were used

    - Similar to other data storages, GCP Datastore also showed a slight increase in latency when the index was configured
      - Final latency whe index configured :

      - Final latency when index not configured :

  - Latency testing in the actual GCP App Engine environment
    - The same conditions were used from the test above
    - During the testing process, 16 out of the 20 thread processes were interrupted and only 4 threads were left to finish the test. The error code was :

    - The final result of the test was :

    - After examining the error situation, I came to a conclusion that since 100,000 entries were simultaneously inserted by a single IP, the App Engine server acknowledged this as an anomaly and the security system blocked most of the threads. Further testing needed to be done after figuring out how to exclude this option if possible
- Noted on the testing process
  - Datastore does not have a function of its own that allows you to delete blocks of data. When testing several times, instead of using a new ‘kind’ keyword to make a new bucket, I wondered if there was a way to use the same bucket but delete and re-insert data
  - Using GCP Dataflow, it was possible to delete data as a whole form Datastore

  - When using Dataflow, it is important that if the ‘kind’ keyword of the bucket you are using in Datastore includes a special case, Dataflow does not recognize that bucket, so if there are plans for deleting data, make sure to use a full-alphabet name 
